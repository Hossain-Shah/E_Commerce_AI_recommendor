# -*- coding: utf-8 -*-
"""ecommerce_content_based_recommendation_engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qV1p68vJqwrnI1De29_Hx38FnkC-vm8F
"""

# importing requirements

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import linear_kernel
ds = pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/products.csv")

# data visualization

ds.head(10)

"""# TF-IDF vectorizer based recommendation(ML)"""

# model process

tf = TfidfVectorizer(analyzer='word', ngram_range=(1, 3), min_df=0, stop_words='english')
tfidf_matrix = tf.fit_transform(ds['description'])

cosine_similarities = linear_kernel(tfidf_matrix, tfidf_matrix)

results = {}

for idx, row in ds.iterrows():
    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]
    similar_items = [(cosine_similarities[idx][i], ds['product_id'][i]) for i in similar_indices]

    results[row['product_id']] = similar_items[1:]

def item(id):
    return ds.loc[ds['product_id'] == id]['description'].tolist()[0].split(' - ')[0]

# Just reads the results out of the dictionary.
def recommend(item_id, num):
    print("Recommending " + str(num) + " products similar to " + item(item_id) + "...")
    print("-------")
    recs = results[item_id][:num]
    for rec in recs:
        print("Recommended: " + item(rec[1]) + " (score:" + str(rec[0]) + ")")

recommend(item_id=11, num=5)

"""# Neural network based recommendation(DL)"""

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, Dense
from sklearn.metrics.pairwise import cosine_similarity

# Load the dataset
ds = pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/products.csv")

# Data preprocessing
descriptions = ds['description'].astype(str).tolist()

# Tokenize the text
tokenizer = Tokenizer()
tokenizer.fit_on_texts(descriptions)
sequences = tokenizer.texts_to_sequences(descriptions)
word_index = tokenizer.word_index

# Pad sequences to ensure uniform input length
max_length = 100
data = pad_sequences(sequences, maxlen=max_length)

# Define model parameters
vocab_size = len(word_index) + 1
embedding_dim = 50

# Define the model
model = Sequential([
    Embedding(vocab_size, embedding_dim, input_length=max_length),
    GlobalAveragePooling1D(),
    Dense(128, activation='relu'),
    Dense(64, activation='relu'),
    Dense(32, activation='relu'),
    Dense(16, activation='relu'),
    Dense(1, activation='sigmoid')
])

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(data, np.ones(len(data)), test_size=0.2, random_state=42)

# Train the model
model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))

# Generate embeddings for each product description
embeddings = model.predict(data)

# Compute cosine similarities
cosine_similarities = cosine_similarity(embeddings)

# Create a dictionary to store the results
results = {}
for idx, row in ds.iterrows():
    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]
    similar_items = [(cosine_similarities[idx][i], ds['product_id'][i]) for i in similar_indices]
    results[row['product_id']] = similar_items[1:]

def item(id):
    return ds.loc[ds['product_id'] == id]['description'].tolist()[0].split(' - ')[0]

# Function to recommend similar products
def recommend(item_id, num):
    print("Recommending " + str(num) + " products similar to " + item(item_id) + "...")
    print("-------")
    recs = results[item_id][:num]
    for rec in recs:
        print("Recommended: " + item(rec[1]) + " (score:" + str(rec[0]) + ")")

recommend(item_id=11, num=5)

"""# Transformer based recommendation(DL)"""

# Importing requirements
import pandas as pd
from sklearn.metrics.pairwise import cosine_similarity
from transformers import DistilBertTokenizer, DistilBertModel
import torch

# Load dataset
ds = pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/products.csv")

# Data visualization
ds.head(10)

# Load pre-trained DistilBERT model and tokenizer
tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')
model = DistilBertModel.from_pretrained('distilbert-base-uncased')

# Function to compute sentence embeddings
def get_embedding(text, tokenizer, model):
    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)
    outputs = model(**inputs)
    return outputs.last_hidden_state.mean(dim=1).detach().numpy()

# Compute embeddings for all product descriptions
embeddings = []
for description in ds['description']:
    embeddings.append(get_embedding(description, tokenizer, model))

embeddings = np.vstack(embeddings)

# Compute cosine similarities
cosine_similarities = cosine_similarity(embeddings, embeddings)

# Create a dictionary to store the results
results = {}

for idx, row in ds.iterrows():
    similar_indices = cosine_similarities[idx].argsort()[:-100:-1]
    similar_items = [(cosine_similarities[idx][i], ds['product_id'][i]) for i in similar_indices]

    results[row['product_id']] = similar_items[1:]

# Function to get product description
def item(id):
    return ds.loc[ds['product_id'] == id]['description'].tolist()[0].split(' - ')[0]

# Function to recommend products
def recommend(item_id, num):
    print("Recommending " + str(num) + " products similar to " + item(item_id) + "...")
    print("-------")
    recs = results[item_id][:num]
    for rec in recs:
        print("Recommended: " + item(rec[1]) + " (score:" + str(rec[0]) + ")")

recommend(item_id=11, num=5)