# -*- coding: utf-8 -*-
"""ecommerce_collaborative_recommendation_engine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1llI5nCyWaQ8-l18wXJuBZ2o-6mDN_hYp
"""

!pip install sentence-transformers

# importing requirements
import pandas as pd
from fastai.collab import CollabDataLoaders, Learner
from fastai.losses import MSELossFlat
import torch.nn as nn
import torch
from sentence_transformers import SentenceTransformer, util

# Load the data with header handling
customer = pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/orders.csv", names=['order_id', 'customer_id'], usecols=[0, 1], skiprows=1)
print("         CUSTOMER        ")
customer.head(10)

product = pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/sales.csv", names=['order_id', 'product_id'], usecols=[1, 2], skiprows=1)
print("         PRODUCT        ")
product.head(10)

product_details = pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/products.csv", names=['product_id', 'description'], usecols=[0, 7], skiprows=1)
print("         PRODUCT DETAILS        ")
product_details.head(10)

# Merge the datasets on 'order_id'
merged_df = product.merge(customer, on='order_id')
print("               SALES            ")
merged_df.head(10)

# Ensure no missing values
merged_df.dropna(inplace=True)

# Remove non-numeric rows
merged_df['product_id'] = pd.to_numeric(merged_df['product_id'], errors='coerce')
merged_df['customer_id'] = pd.to_numeric(merged_df['customer_id'], errors='coerce')
merged_df = merged_df.dropna()

# Convert columns to integers
merged_df['product_id'] = merged_df['product_id'].astype(int)
merged_df['customer_id'] = merged_df['customer_id'].astype(int)

# Ensure that the DataFrame has only the required columns
merged_df = merged_df[['customer_id', 'product_id']]

# Add a dummy rating column
merged_df['rating'] = 1
merged_with_details = merged_df.merge(product_details, on='product_id', how='left')

# Processed DataFrame
print("           PROCESSED SALES DATA            ")
merged_with_details.head(10)

# Create data loaders
dls = CollabDataLoaders.from_df(merged_with_details, item_name='description', user_name='customer_id', rating_name='rating', bs=64)

# Display a batch of data
dls.show_batch()

# Number of products and customers
n_products = len(dls.classes['description'])
n_customers = len(dls.classes['customer_id'])
print(f"Number of products: {n_products}")
print(f"Number of customers: {n_customers}")
print("DataFrame shape:", merged_df.shape)

# Define the model
class DotProductBias(nn.Module):
    def __init__(self, n_products, n_customers, n_factors, y_range=(0, 5.5)):
        super().__init__()
        self.product_factors = nn.Embedding(n_products, n_factors)
        self.product_bias = nn.Embedding(n_products, 1)
        self.customer_factors = nn.Embedding(n_customers, n_factors)
        self.customer_bias = nn.Embedding(n_customers, 1)
        self.y_range = y_range

    def forward(self, x):
        product_indices = x[:, 1]
        customer_indices = x[:, 0]

        products = self.product_factors(product_indices)
        customers = self.customer_factors(customer_indices)

        dot_product = (products * customers).sum(1)
        bias = self.product_bias(product_indices).squeeze() + self.customer_bias(customer_indices).squeeze()

        return torch.sigmoid(dot_product + bias) * (self.y_range[1] - self.y_range[0]) + self.y_range[0]

# Initialize model
model = DotProductBias(n_products, n_customers, 50)

# Create Learner
learn = Learner(dls, model, loss_func=MSELossFlat())

# Try running on CPU first
learn.model = learn.model.cpu()

# Fit the model
learn.fit_one_cycle(5, 5e-3, wd=0.1)
learn.save('/content/drive/MyDrive/Colab_Notebooks/dot_product_model')

# Extract product biases
product_bias = learn.model.product_bias.weight.squeeze()
idxs = product_bias.argsort()[:5]
lowest_bias_products = [dls.classes['description'][i] for i in idxs]

print("Products with the lowest biases:", lowest_bias_products)

# Extract product biases in descending order
idxs = product_bias.argsort(descending=True)[:5]
highest_bias_products = [dls.classes['description'][i] for i in idxs]

print("Products with the highest biases:", highest_bias_products)

"""# TRANSFORMER BASED RECOMMENDATION(DL)"""

# Load SentenceTransformer model for description embeddings
embedder = SentenceTransformer('all-MiniLM-L6-v2')

# Compute embeddings for all product descriptions
descriptions = dls.classes['description']
description_embeddings = embedder.encode(descriptions, convert_to_tensor=True)

def find_similar_products(input_description, top_n=5):
    # Encode the input description
    input_embedding = embedder.encode(input_description, convert_to_tensor=True)

    # Compute cosine similarities
    cosine_scores = util.pytorch_cos_sim(input_embedding, description_embeddings)[0]

    # Find the top_n most similar descriptions
    top_results = torch.topk(cosine_scores, k=top_n)

    similar_products = []
    for score, idx in zip(top_results[0], top_results[1]):
        similar_products.append({
            'description': descriptions[idx],
            'score': score.item()
        })

    return similar_products

# Example usage
input_description = "A yellow coloured, S sized, Windbreaker Jacket"
similar_products = find_similar_products(input_description)
print("Similar products:", similar_products)

"""# DOT PRODUCT BASED COLLABORATIVE FILTERING(ML)"""

# Extract product embeddings
product_factors = model.product_factors.weight.detach().cpu()

def find_similar_products(product_id, top_n=5):
    # Get the index of the product_id
    product_idx = dls.classes['description'].o2i[product_id]

    # Get the product embedding
    product_embedding = product_factors[product_idx]

    # Compute cosine similarities
    cosine_scores = torch.nn.functional.cosine_similarity(product_embedding.unsqueeze(0), product_factors)

    # Find the top_n most similar products
    top_results = torch.topk(cosine_scores, k=top_n+1)  # top_n+1 because the product will be most similar to itself

    similar_products = []
    for score, idx in zip(top_results[0][1:], top_results[1][1:]):  # skip the first one because it's the product itself
        similar_products.append({
            'description': dls.classes['description'][idx],
            'score': score.item()
        })

    return similar_products

# Example usage
example_product_id = 'A yellow coloured, S sized, Windbreaker Jacket'
similar_products = find_similar_products(example_product_id)
print("Similar products:", similar_products)